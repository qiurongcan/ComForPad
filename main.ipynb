{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrackA 下载官方数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = r'Dataset/Testset_track_A/Inference/'\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    print(\"数据文件夹存在\")\n",
    "else:\n",
    "    !wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; HSID=A-4I-ZudDNUIB6EKH; SSID=A7v_1v9un6xAwVNku; APISID=ctK8IbLjeuDUmgys/AFnMSLWt9KddceDI6; SAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; __Secure-1PAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; __Secure-3PAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; SID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_kzuBV1TvOhAIC8VF1e9fpgACgYKATQSARQSFQHGX2Mi8LXUwWoIwNCEPU8Sy3mXUxoVAUF8yKqGXVfjTGz9gQal7nwGr4Pl0076; __Secure-1PSID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_PDa-DzVmbdGFPyxMQpk9_QACgYKAewSARQSFQHGX2MiAeee4fn0OWglWZfAygqkyBoVAUF8yKp-Sfmtnueimxc-0QbJRF9I0076; __Secure-3PSID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_g9IrMeU98APBo9Stp6wEnAACgYKAQASARQSFQHGX2MiFWtc9ucONXnpxBzlRdudEhoVAUF8yKoeZwCpJDnjfAFjGssHSUGm0076; NID=515=GQhY9nKKFCx3qFDjE0MA4ubjWNdef6xCIY_RfWOPWKEtyfBN3nAUl8WHI2VczjNQ4rVkj1XBAY8WNWHXyqSK10CfT4FxsFlPzrHIJpeTtm1nWRNBd9AAfBKJHz4XpESszntVUTE_59RklZuKo0vk1poReVi2da1PZKC3CTKH2Ll3gB5xuB9wf4bmq8ylVUuIROPJczr0XnCuUHV3qLdBvgy9_870b6UwOq1iOlIxFQFm01EZ4pqF4q1Ub3QRSWpEMLh4LSZFpJ5O255R5OV7krmEdDvH_sHoTEPZAg2PoEpwAyGK6Xp9qcLIlldgx5-5V86N8Wtb93uTlQuA_CFXb5_2eP3bgeX8txwlJ5SrldVjg9ctzYtBU2RwJKTSvdHfIG7lpOkg6XlkvDOcJpR3DihT_OlqnPn7drCAJpvVDv29hZn5XPMXaSrNdbG64OJ9urJEw5odEwsLYkkpC1vmlUcuoo52S5f6RQu0Z8kZiV8iRW6XIqHsSmQHunVaxk6xWCStUg; __Secure-1PSIDTS=sidts-CjEB3EgAEtTS0OazynCofIH4RCBstiRP5flEcvYW3z4Fg9oGd5QOESDOZt1wO2iqUYHjEAA; __Secure-3PSIDTS=sidts-CjEB3EgAEtTS0OazynCofIH4RCBstiRP5flEcvYW3z4Fg9oGd5QOESDOZt1wO2iqUYHjEAA; SIDCC=AKEyXzVI6aMX8lSDja86Yts3FBAtBzPCzVNgaX5BCz78NWsWzlT3yFWKUV7ZE46SFzE1GiBI-cHdTw; __Secure-1PSIDCC=AKEyXzUo4NQAwqqPMxP2eye-MFEbZmBIm_sZqRU1amttg0YoQkc8ZKSNXdHl5jNCMEbhrUHhS9-K; __Secure-3PSIDCC=AKEyXzWf2lIdmDLeZKpXSi9GytVQb6XudrYiNUBA5gW952YuLh8kL6T3IbBlu8zOTfGEcdUp5O1R\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1JwR0Q1ArTg6c47EF2ZuIBpQwCPgXKrO2&export=download&authuser=0&confirm=t&uuid=dc3aa13c-c3a9-458f-983a-8586798cb635&at=APZUnTX25XMxi-z-3wBcgR93IGsL%3A1719235792953\" -c -O \"Dataset.zip\"\n",
    "    !unzip Dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装代码需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip3 install open3d\n",
    "!pip3 install timm\n",
    "!pip3 install einops\n",
    "!pip3 install unzip\n",
    "!pip3 install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练最优权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'id' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "# 加载权重\n",
    "# best_weightsA.pt\n",
    "# https://drive.google.com/file/d/1-Bo2wR4lJ1ZiHDF4EbhhgbE2ky9Dcuw9/view?usp=sharing\n",
    "!wget  --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-Bo2wR4lJ1ZiHDF4EbhhgbE2ky9Dcuw9' -O best_weightsA.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器\n",
    "# 加载测试集\n",
    "# 代码模型\n",
    "import numpy as np\n",
    "import open3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# 按照顺序读取标签\n",
    "def read_order(path):\n",
    "    path = os.path.join(path, 'watertight_meshes.txt')\n",
    "    order = []\n",
    "    with open(path,mode='r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # 需要清除换行符\n",
    "            order.append(line.strip())\n",
    "    return order        \n",
    "\n",
    "# 加载特征和标签的路径\n",
    "def load_train(path=r'Dataset/Training_data'):\n",
    "    order = read_order(path=path)\n",
    "    feature_path = []\n",
    "    label_path = []\n",
    "    f_base_path = os.path.join(path, 'Feature')\n",
    "    l_base_path = os.path.join(path, 'Label')\n",
    "    for o in order:\n",
    "        f_path = f_base_path + f\"/mesh_{o}.ply\"\n",
    "        l_path = l_base_path + f\"/press_{o}.npy\"\n",
    "        feature_path.append(f_path)\n",
    "        label_path.append(l_path)\n",
    "    return feature_path, label_path\n",
    "\n",
    "\n",
    "\n",
    "class MeshPressDataset(Dataset):\n",
    "    def __init__(self, feature_dir, press_dir):\n",
    "        # 特征和标签的文件夹\n",
    "        self.feature_dir = feature_dir\n",
    "        self.press_dir = press_dir\n",
    "        # 判读长度是否一致\n",
    "        assert len(self.feature_dir) == len(self.press_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_dir)\n",
    "    \n",
    "    # 创造一个迭代器获取数据集\n",
    "    def __getitem__(self, idx):\n",
    "        pcd = open3d.io.read_point_cloud(self.feature_dir[idx])\n",
    "        # 转换为np数据\n",
    "        vertices = np.array(pcd.points)\n",
    "        # 在此处进行裁剪压力\n",
    "        # print(self.label_paths[idx])\n",
    "        press = np.load(self.press_dir[idx])\n",
    "        press = np.concatenate((press[:16], press[112:]), axis=0)\n",
    "        # 确保数据长度一致\n",
    "        assert vertices.shape[0] == press.shape[0]\n",
    "\n",
    "        return torch.tensor(vertices, dtype=torch.float32), torch.tensor(press, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "def read_test(path=r'Dataset/Testset_track_A/Inference/'):\n",
    "    \"\"\"需要传入测试集文件夹路径\"\"\"\n",
    "    t_base_path = path\n",
    "    paths = os.listdir(t_base_path)\n",
    "    test_path = []\n",
    "    for p in paths:\n",
    "        t_path = t_base_path + p\n",
    "        test_path.append(t_path)\n",
    "    return test_path\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir):\n",
    "        self.test_dir = test_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.test_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pcd = open3d.io.read_point_cloud(self.test_dir[idx])\n",
    "        # 转换为np数据\n",
    "        vertices = np.array(pcd.points)\n",
    "\n",
    "        return torch.tensor(vertices, dtype=torch.float32), self.test_dir[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载损失函数\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# 定义损失函数\n",
    "class LpLoss(nn.Module):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction='mean'):\n",
    "        super(LpLoss, self).__init__()\n",
    "        assert d > 0 and p > 0\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        num_examples = x.size(0)\n",
    "        h = 1.0 / (x.size(1) - 1.0)\n",
    "        all_norms = (h ** (self.d / self.p)) * torch.norm(\n",
    "            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, dim=1\n",
    "        )\n",
    "        if self.reduction == 'mean':\n",
    "            return all_norms.mean() if self.size_average else all_norms.sum()\n",
    "        elif self.reduction == 'sum':\n",
    "            return all_norms.sum()\n",
    "        else:\n",
    "            return all_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Transolver模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "ACTIVATION = {'gelu': nn.GELU, 'tanh': nn.Tanh, 'sigmoid': nn.Sigmoid, 'relu': nn.ReLU, 'leaky_relu': nn.LeakyReLU(0.1),\n",
    "              'softplus': nn.Softplus, 'ELU': nn.ELU, 'silu': nn.SiLU}\n",
    "\n",
    "\n",
    "class Physics_Attention_Irregular_Mesh(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0., slice_num=64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.dim_head = dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.temperature = nn.Parameter(torch.ones([1, heads, 1, 1]) * 0.5)\n",
    "\n",
    "        self.in_project_x = nn.Linear(dim, inner_dim)\n",
    "        self.in_project_fx = nn.Linear(dim, inner_dim)\n",
    "        self.in_project_slice = nn.Linear(dim_head, slice_num)\n",
    "        for l in [self.in_project_slice]:\n",
    "            torch.nn.init.orthogonal_(l.weight)  # use a principled initialization\n",
    "        self.to_q = nn.Linear(dim_head, dim_head, bias=False)\n",
    "        self.to_k = nn.Linear(dim_head, dim_head, bias=False)\n",
    "        self.to_v = nn.Linear(dim_head, dim_head, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B N C\n",
    "        # print(x.shape)\n",
    "        _, B, N, C = x.shape\n",
    "        \n",
    "        ### (1) Slice\n",
    "        fx_mid = self.in_project_fx(x).reshape(B, N, self.heads, self.dim_head) \\\n",
    "            .permute(0, 2, 1, 3).contiguous()  # B H N C\n",
    "        x_mid = self.in_project_x(x).reshape(B, N, self.heads, self.dim_head) \\\n",
    "            .permute(0, 2, 1, 3).contiguous()  # B H N C\n",
    "        slice_weights = self.softmax(self.in_project_slice(x_mid) / self.temperature)  # B H N G\n",
    "        slice_norm = slice_weights.sum(2)  # B H G\n",
    "        slice_token = torch.einsum(\"bhnc,bhng->bhgc\", fx_mid, slice_weights)\n",
    "        slice_token = slice_token / ((slice_norm + 1e-5)[:, :, :, None].repeat(1, 1, 1, self.dim_head))\n",
    "\n",
    "        ### (2) Attention among slice tokens\n",
    "        q_slice_token = self.to_q(slice_token)\n",
    "        k_slice_token = self.to_k(slice_token)\n",
    "        v_slice_token = self.to_v(slice_token)\n",
    "        dots = torch.matmul(q_slice_token, k_slice_token.transpose(-1, -2)) * self.scale\n",
    "        attn = self.softmax(dots)\n",
    "        attn = self.dropout(attn)\n",
    "        out_slice_token = torch.matmul(attn, v_slice_token)  # B H G D\n",
    "\n",
    "        ### (3) Deslice\n",
    "        out_x = torch.einsum(\"bhgc,bhng->bhnc\", out_slice_token, slice_weights)\n",
    "        out_x = rearrange(out_x, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out_x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu', res=True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if act in ACTIVATION.keys():\n",
    "            act = ACTIVATION[act]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.n_layers = n_layers\n",
    "        self.res = res\n",
    "        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act())\n",
    "        self.linear_post = nn.Linear(n_hidden, n_output)\n",
    "        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_pre(x)\n",
    "        for i in range(self.n_layers):\n",
    "            if self.res:\n",
    "                x = self.linears[i](x) + x\n",
    "            else:\n",
    "                x = self.linears[i](x)\n",
    "        x = self.linear_post(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transolver_block(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_heads: int,\n",
    "            hidden_dim: int,\n",
    "            dropout: float,\n",
    "            act='gelu',\n",
    "            mlp_ratio=4,\n",
    "            last_layer=False,\n",
    "            out_dim=1,\n",
    "            slice_num=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.last_layer = last_layer\n",
    "        self.ln_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.Attn = Physics_Attention_Irregular_Mesh(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads,\n",
    "                                                     dropout=dropout, slice_num=slice_num)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = MLP(hidden_dim, hidden_dim * mlp_ratio, hidden_dim, n_layers=0, res=False, act=act)\n",
    "        if self.last_layer:\n",
    "            self.ln_3 = nn.LayerNorm(hidden_dim)\n",
    "            self.mlp2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, fx):\n",
    "        fx = self.Attn(self.ln_1(fx)) + fx\n",
    "        fx = self.mlp(self.ln_2(fx)) + fx\n",
    "        if self.last_layer:\n",
    "            return self.mlp2(self.ln_3(fx))\n",
    "        else:\n",
    "            return fx\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 space_dim=1,\n",
    "                 n_layers=5,\n",
    "                 n_hidden=256,\n",
    "                 dropout=0,\n",
    "                 n_head=8,\n",
    "                 act='gelu',\n",
    "                 mlp_ratio=1,\n",
    "                 fun_dim=1,\n",
    "                 out_dim=1,\n",
    "                 slice_num=32,\n",
    "                 ref=8,\n",
    "                 unified_pos=False\n",
    "                 ):\n",
    "        super(Model, self).__init__()\n",
    "        self.__name__ = 'Transolver'\n",
    "        self.ref = ref\n",
    "        self.unified_pos = unified_pos\n",
    "        if self.unified_pos:\n",
    "            self.preprocess = MLP(fun_dim + self.ref * self.ref * self.ref, n_hidden * 2, n_hidden, n_layers=0,\n",
    "                                  res=False, act=act)\n",
    "        else:\n",
    "            self.preprocess = MLP(fun_dim + space_dim, n_hidden * 2, n_hidden, n_layers=0, res=False, act=act)\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.space_dim = space_dim\n",
    "\n",
    "        self.blocks = nn.ModuleList([Transolver_block(num_heads=n_head, hidden_dim=n_hidden,\n",
    "                                                      dropout=dropout,\n",
    "                                                      act=act,\n",
    "                                                      mlp_ratio=mlp_ratio,\n",
    "                                                      out_dim=out_dim,\n",
    "                                                      slice_num=slice_num,\n",
    "                                                      last_layer=(_ == n_layers - 1))\n",
    "                                     for _ in range(n_layers)])\n",
    "        self.initialize_weights()\n",
    "        self.placeholder = nn.Parameter((1 / (n_hidden)) * torch.rand(n_hidden, dtype=torch.float))\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_grid(self, my_pos):\n",
    "        # my_pos 1 N 3\n",
    "        batchsize = my_pos.shape[0]\n",
    "\n",
    "        gridx = torch.tensor(np.linspace(-1.5, 1.5, self.ref), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, self.ref, 1, 1, 1).repeat([batchsize, 1, self.ref, self.ref, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 2, self.ref), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, self.ref, 1, 1).repeat([batchsize, self.ref, 1, self.ref, 1])\n",
    "        gridz = torch.tensor(np.linspace(-4, 4, self.ref), dtype=torch.float)\n",
    "        gridz = gridz.reshape(1, 1, 1, self.ref, 1).repeat([batchsize, self.ref, self.ref, 1, 1])\n",
    "        grid_ref = torch.cat((gridx, gridy, gridz), dim=-1).cuda().reshape(batchsize, self.ref ** 3, 3)  # B 4 4 4 3\n",
    "\n",
    "        pos = torch.sqrt(\n",
    "            torch.sum((my_pos[:, :, None, :] - grid_ref[:, None, :, :]) ** 2,\n",
    "                      dim=-1)). \\\n",
    "            reshape(batchsize, my_pos.shape[1], self.ref * self.ref * self.ref).contiguous()\n",
    "        return pos\n",
    "\n",
    "    def forward(self, data):\n",
    "        cfd_data = data\n",
    "        x, fx = cfd_data, None\n",
    "        x = x[None, :, :]\n",
    "        # 没有unified_pos\n",
    "        if self.unified_pos:\n",
    "            new_pos = self.get_grid(cfd_data.pos[None, :, :])\n",
    "            x = torch.cat((x, new_pos), dim=-1)\n",
    "\n",
    "        if fx is not None:\n",
    "            fx = torch.cat((x, fx), -1)\n",
    "            fx = self.preprocess(fx)\n",
    "        else:\n",
    "            fx = self.preprocess(x)\n",
    "            fx = fx + self.placeholder[None, None, :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            fx = block(fx)\n",
    "\n",
    "        return fx[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义超参数函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型超参数\n",
    "import argparse\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"use config param for Transovler 7.11\")\n",
    "\n",
    "    # --------------------初始化各个参数------------------------------\n",
    "    # training Setting\n",
    "    parser.add_argument('--epochs', type=int, default=300, help=\"set the train epochs\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"maybe 0.001 is large\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1225, help=\"Birthday\")\n",
    "    parser.add_argument(\"--log_dir\", default=\"train100\", help=\"setting the save file name\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=8, help=\"for training\")\n",
    "    parser.add_argument(\"--train_ratio\", type=float, default=0.98, help=\"500*0.98 -> 490:10\")\n",
    "    parser.add_argument(\"--val_batch\", type=int, default=4, help=\"default=1\")\n",
    "    parser.add_argument(\"--max_lr\", type=float, default=0.001, help=\"use OneCycle method\")\n",
    "    parser.add_argument('--train_path', default='Dataset/Training_data')\n",
    "    # model Setting\n",
    "    parser.add_argument(\"--n_layers\", type=int, default=7, help=\"maybe 8 is best\")\n",
    "    parser.add_argument(\"--n_hidden\", type=int, default=256, help=\"\")\n",
    "    parser.add_argument(\"--space_dim\", type=int, default=3, help=\"input data's dim, in trackA is 3\")\n",
    "    parser.add_argument(\"--mlp_ratio\", type=int, default=2, help='2 is best? maybe')\n",
    "    parser.add_argument(\"--fun_dim\", type=int, default=0, help=\"set 0 !!\")\n",
    "    parser.add_argument(\"--n_head\", type=int, default=8, help='atten head')\n",
    "    parser.add_argument(\"--slice_num\", type=int, default=32, help='physic atten slice')\n",
    "    parser.add_argument(\"--frezze\", action='store_true', help='frezze some params')\n",
    "    parser.add_argument('--fre_layers', type=list, default=None, help='choose fre some layer')\n",
    "    parser.add_argument('--fre_epoch', type=int, default=300, help='star frezee')\n",
    "    parser.add_argument(\"--act\", default='gelu', help=\"activate function\", choices=['gelu', 'tanh', 'sigmoid', 'relu', 'leaky_relu', 'softplus', 'ELU', 'silu'])\n",
    "\n",
    "    parser.add_argument('--result', default='best', choices=['best', 'last'], help=\"use best or last weights\")\n",
    "    parser.add_argument('--output_file', default=\"B_Results\", help='need to submit')\n",
    "    parser.add_argument('--is_zip', default=1, choices=[0, 1], help='zip or not zip')\n",
    "    parser.add_argument('--test_path', default=r'Dataset/Testset_track_A/Inference/', help='TestSet Path')\n",
    "\n",
    "    # opt = parser.parse_args()\n",
    "    opt = parser.parse_known_args()[0]\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练代码，并保存权重文件在runs_trakcA/train100目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------训练模型------------------------------\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import random_split,DataLoader\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=1225):\n",
    "    # 固定随机种子\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    msg =f\"\"\"\n",
    "    =============================================\n",
    "        seed = {seed}\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    =============================================\n",
    "    \"\"\"\n",
    "    print(msg)\n",
    "    \n",
    "def judge_file(folder_path='runs_trackA'):\n",
    "    folder_path = folder_path\n",
    "\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        print(\"文件夹存在\")\n",
    "    else:\n",
    "        os.system(f\"mkdir -p {folder_path}\")\n",
    "\n",
    "def train(opt):\n",
    "\n",
    "    log_p = opt.log_dir\n",
    "    # 设置随机种子\n",
    "    set_seed(opt.seed)\n",
    "    train_split = opt.train_ratio\n",
    "    train_batch = opt.batch\n",
    "    val_batch = opt.val_batch\n",
    "    num_epoch = opt.epochs\n",
    "    n_layers = opt.n_layers\n",
    "    n_hidden = opt.n_hidden\n",
    "    space_dim = opt.space_dim\n",
    "    mlp_ratio = opt.mlp_ratio\n",
    "    fun_dim = opt.fun_dim\n",
    "    lr = opt.lr\n",
    "    max_lr = opt.max_lr\n",
    "    act = opt.act\n",
    "    slice_num = opt.slice_num\n",
    "    n_head = opt.n_head\n",
    "    \n",
    "    # 创建保存权重的文件夹\n",
    "    save_file = 'runs_trackA'\n",
    "    judge_file(save_file)\n",
    "    log_file = os.path.join(save_file, log_p)\n",
    "    judge_file(log_file)\n",
    "\n",
    "    # 保存训练过程到csv文件中\n",
    "    f = open(f'{log_file}/train_process.csv', mode='w')\n",
    "    fc = csv.writer(f)\n",
    "    fc.writerow([\"train_loss\", \"val_loss\", 'lr'])\n",
    "\n",
    "    with open(f'{log_file}/args.txt', mode='w') as fargs:\n",
    "        fargs.write(str(opt))\n",
    "    \n",
    "    print(f\"saving args to {log_file}\")\n",
    "    \n",
    "    # 1. -加载数据集 百度给的数据集\n",
    "    feature_paths, label_paths = load_train()\n",
    "    dataset = MeshPressDataset(feature_paths, label_paths)\n",
    "    \n",
    "\n",
    "    # 检查gpu是否可用\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available, using CPU\")\n",
    "\n",
    "    \n",
    "    # 2. -划分数据集为训练集和验证集 0.8 : 0.2\n",
    "    # 可以试一下 0.9:0.1 可能效果会好很多\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "    # 3. -创建数据集加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_batch, shuffle=False)\n",
    "\n",
    "\n",
    "    # 4. -实例化模型\n",
    "    model = Model(\n",
    "        n_layers=n_layers, \n",
    "        n_hidden=n_hidden, \n",
    "        space_dim=space_dim, \n",
    "        mlp_ratio=mlp_ratio, \n",
    "        fun_dim=fun_dim,\n",
    "        act=act,\n",
    "        n_head=n_head,\n",
    "        slice_num=slice_num\n",
    "        )\n",
    "\n",
    "        \n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"---模型的参数为：{params} ---\")\n",
    "    \n",
    "\n",
    "    # 5. -定义损失函数\n",
    "    criterion = LpLoss(size_average=True)\n",
    "\n",
    "    # 6. -定义优化器 修改不同的参数，就不能加载预训练权重，会重新开始训练，最好的方法就是修改存储weights的文件夹的名称 学习率不能太大，太难了乱跑 学习率需要小一些 下降的速度也需要慢一点，太快了容易提前收敛了\n",
    "    #  之后需要修改lr lr_decay,获取弄的比较好的效果\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.00005, betas=(0.9, 0.999))\n",
    "    \n",
    "    # 7. -开始训练\n",
    "    best_val_loss = 1000\n",
    "    best_train_loss = 1000\n",
    "    # 把学习率下降调小\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,\n",
    "        epochs=num_epoch,\n",
    "        div_factor=1e4, pct_start=0.25, final_div_factor=1e4,\n",
    "        steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    \n",
    "    # scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.000001, max_lr=0.0008, step_size_up=800, mode='triangular')\n",
    "    grad_clip = 1000.0\n",
    "    for epoch in range(num_epoch):\n",
    "        t1 = time()\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs,y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            # break\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        \n",
    "\n",
    "        # 8. -保存模型最后一轮权重\n",
    "        last_weights_path = f'{log_file}/last_weights.pt'\n",
    "        torch.save(model.state_dict(),last_weights_path)\n",
    "\n",
    "        # 9. -保存最优权重\n",
    "        if val_loss <= best_val_loss and train_loss <= best_train_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_train_loss = train_loss\n",
    "            best_weight_path = f'{log_file}/best_weights.pt'\n",
    "            torch.save(model.state_dict(), best_weight_path)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        \n",
    "        if epoch+1 == opt.fre_epoch:\n",
    "            if opt.frezze:\n",
    "                # if opt.fre_epoch <= opt.epochs:\n",
    "                \n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "                \n",
    "                for fre_layer in opt.fre_layers:\n",
    "                    # print(fre_layer)\n",
    "                    # 冻结选择的层数\n",
    "                    for param in model.blocks[int(fre_layer)].parameters():\n",
    "                        param.requires_grad = False\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=current_lr, weight_decay=0.00005, betas=(0.9, 0.999))\n",
    "                scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                    optimizer,\n",
    "                    max_lr=current_lr,\n",
    "                    epochs=num_epoch-opt.fre_epoch,\n",
    "                    div_factor=1e4, pct_start=0.001, final_div_factor=1e4,\n",
    "                    steps_per_epoch=len(train_loader)\n",
    "                )\n",
    "        \n",
    "        t2 =time()\n",
    "        t = t2 - t1\n",
    "        print(f'Epoch {epoch+1}/{num_epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Current_lr: {current_lr}, Spend Time: {round(t,2)}s')\n",
    "        fc.writerow([train_loss, val_loss, current_lr])\n",
    "\n",
    "    f.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行测试集代码，并把结果输出到B_Results.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------测试结果-------------------------------\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def test(opt, flag='test'):\n",
    "    log = opt.log_dir\n",
    "    mode = opt.result\n",
    "    output_file = opt.output_file\n",
    "    # 检查gpu是否可用\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available, using CPU\")\n",
    "\n",
    "    # 加载模型，进行测试\n",
    "    n_layers = opt.n_layers\n",
    "    n_hidden = opt.n_hidden\n",
    "    space_dim = opt.space_dim\n",
    "    mlp_ratio = opt.mlp_ratio\n",
    "    fun_dim = opt.fun_dim\n",
    "    act = opt.act\n",
    "    slice_num = opt.slice_num\n",
    "    n_head = opt.n_head\n",
    "    model = Model(\n",
    "        n_layers=n_layers, \n",
    "        n_hidden=n_hidden, \n",
    "        space_dim=space_dim, \n",
    "        mlp_ratio=mlp_ratio, \n",
    "        fun_dim=fun_dim,\n",
    "        act=act,\n",
    "        n_head=n_head,\n",
    "        slice_num=slice_num,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # os.system(f\"mkdir -p runs_trackA/{log}/{output_file}\")\n",
    "    # 直接在当前路径下生成一个\n",
    "    os.system(f'mkdir -p {output_file}')\n",
    "\n",
    "    # 加载最优的权重模型\n",
    "    if mode == 'best':\n",
    "        if flag == 'train':\n",
    "            best_weights_path = f'runs_trackA/{log}/best_weights.pt'\n",
    "        else:\n",
    "            best_weights_path = r'best_weightsA.pt'  # 直接在当前目录下读取最佳权重\n",
    "        model.load_state_dict(torch.load(best_weights_path))\n",
    "    \n",
    "      \n",
    "    # 加载到GPU上\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    # 获取测试集的路径\n",
    "    test_path = read_test(path=opt.test_path)\n",
    "    # 加载测试集数据\n",
    "    dataset = TestDataset(test_dir=test_path)\n",
    "    test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # 进行测试\n",
    "    for X_batch, p in test_loader:\n",
    "\n",
    "        # 将数据部署到GPU上\n",
    "        X_batch= X_batch.to(device)        \n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        # 将输出到cpu上\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        # print(type(outputs))\n",
    "        # 数据进行展平，并转换为numpy的格式\n",
    "        outputs = outputs.detach().numpy().flatten()\n",
    "        print(np.max(outputs))\n",
    "\n",
    "        # 处理保存数据的路径 \n",
    "        number = str(p).split(\".\")[0][-3:]\n",
    "        path = f'{output_file}/press_{number}.npy'\n",
    "\n",
    "        # 保存数据\n",
    "        np.save(path,outputs)\n",
    "\n",
    "    print(\"-----完成数据的保存啦～=～-------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练主函数，并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--lr LR] [--seed SEED]\n",
      "                             [--log_dir LOG_DIR] [--batch BATCH]\n",
      "                             [--train_ratio TRAIN_RATIO]\n",
      "                             [--val_batch VAL_BATCH] [--max_lr MAX_LR]\n",
      "                             [--train_path TRAIN_PATH] [--n_layers N_LAYERS]\n",
      "                             [--n_hidden N_HIDDEN] [--space_dim SPACE_DIM]\n",
      "                             [--mlp_ratio MLP_RATIO] [--fun_dim FUN_DIM]\n",
      "                             [--n_head N_HEAD] [--slice_num SLICE_NUM]\n",
      "                             [--frezze] [--fre_layers FRE_LAYERS]\n",
      "                             [--fre_epoch FRE_EPOCH]\n",
      "                             [--act {gelu,tanh,sigmoid,relu,leaky_relu,softplus,ELU,silu}]\n",
      "                             [--result {best,last}]\n",
      "                             [--output_file OUTPUT_FILE] [--is_zip {0,1}]\n",
      "                             [--test_path TEST_PATH]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\xjtu\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-24136VmdE3o7m90EU.json could match --fun_dim, --frezze, --fre_layers, --fre_epoch\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xjtu\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    opt = get_args()\n",
    "    train(opt=opt)\n",
    "    test(opt=opt, flag='train')\n",
    "    # 压缩文件\n",
    "    if opt.is_zip:\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile('B_Results.zip','w') as zipObj:\n",
    "            file_list = os.listdir('B_Results')\n",
    "            for f in file_list:\n",
    "                p = os.path.join('B_Results/', f)\n",
    "                zipObj.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试最优权重，从google drive上下载的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集数据\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    opt = get_args()\n",
    "    test(opt)\n",
    "\n",
    "    # 压缩文件\n",
    "    if opt.is_zip:\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile('B_Results.zip','w') as zipObj:\n",
    "            file_list = os.listdir('B_Results')\n",
    "            for f in file_list:\n",
    "                p = os.path.join('B_Results/', f)\n",
    "                zipObj.write(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
